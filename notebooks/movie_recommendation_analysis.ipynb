{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bd2ae22",
   "metadata": {},
   "source": [
    "# Movie Recommendation System Analysis ðŸŽ¬\n",
    "\n",
    "This notebook implements a comprehensive movie recommendation system using collaborative filtering, similarity matrices, and machine learning models. We'll build a system that predicts user ratings and recommends movies using the MovieLens 20M dataset.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "**Goals:**\n",
    "- Build a collaborative filtering system using user-user and item-item similarity\n",
    "- Predict user ratings with high accuracy (target RMSE < 0.85)\n",
    "- Create intelligent features for machine learning models\n",
    "- Compare multiple ML models and select the best performer\n",
    "- Generate personalized movie recommendations\n",
    "\n",
    "**Dataset:** MovieLens 20M\n",
    "- 20 million ratings\n",
    "- 138,000 users\n",
    "- 27,000 movies\n",
    "\n",
    "**Methods:**\n",
    "1. **Similarity-based Collaborative Filtering**\n",
    "2. **Feature Engineering & Machine Learning**\n",
    "3. **Hybrid Recommendation System**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ea6d8e",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Load Data ðŸ“š\n",
    "\n",
    "Let's import all the necessary libraries for data analysis, machine learning, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc8166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Advanced ML libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    import lightgbm as lgb\n",
    "    print(\"âœ… XGBoost and LightGBM available\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ XGBoost or LightGBM not available - install with pip install xgboost lightgbm\")\n",
    "\n",
    "# Plotting setup\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"ðŸ“š All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2624434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "DATA_PATH = Path(\"../data\")\n",
    "RAW_DATA_PATH = DATA_PATH / \"raw\"\n",
    "PROCESSED_DATA_PATH = DATA_PATH / \"processed\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "PROCESSED_DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_movielens_data():\n",
    "    \"\"\"Load MovieLens dataset from CSV files.\"\"\"\n",
    "    try:\n",
    "        # Load ratings data\n",
    "        ratings_path = RAW_DATA_PATH / \"ratings.csv\"\n",
    "        if ratings_path.exists():\n",
    "            ratings_df = pd.read_csv(ratings_path)\n",
    "            print(f\"âœ… Loaded {len(ratings_df):,} ratings\")\n",
    "        else:\n",
    "            print(\"âš ï¸ ratings.csv not found in data/raw/\")\n",
    "            print(\"ðŸ“¥ Please download MovieLens 20M dataset from:\")\n",
    "            print(\"   https://grouplens.org/datasets/movielens/20m/\")\n",
    "            # Create sample data for demonstration\n",
    "            print(\"ðŸ”§ Creating sample data for demonstration...\")\n",
    "            ratings_df = create_sample_data()\n",
    "        \n",
    "        # Load movies data\n",
    "        movies_path = RAW_DATA_PATH / \"movies.csv\"\n",
    "        if movies_path.exists():\n",
    "            movies_df = pd.read_csv(movies_path)\n",
    "            print(f\"âœ… Loaded {len(movies_df):,} movies\")\n",
    "        else:\n",
    "            print(\"âš ï¸ movies.csv not found - creating sample movie data\")\n",
    "            movies_df = create_sample_movies(ratings_df['movieId'].unique())\n",
    "        \n",
    "        return ratings_df, movies_df\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading data: {e}\")\n",
    "        print(\"ðŸ”§ Creating sample data...\")\n",
    "        ratings_df = create_sample_data()\n",
    "        movies_df = create_sample_movies(ratings_df['movieId'].unique())\n",
    "        return ratings_df, movies_df\n",
    "\n",
    "def create_sample_data():\n",
    "    \"\"\"Create sample ratings data for demonstration.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_users, n_movies, n_ratings = 5000, 1000, 50000\n",
    "    \n",
    "    # Generate realistic user-movie interactions\n",
    "    user_ids = np.random.choice(range(1, n_users + 1), n_ratings)\n",
    "    movie_ids = np.random.choice(range(1, n_movies + 1), n_ratings)\n",
    "    \n",
    "    # Create realistic rating distribution (skewed toward higher ratings)\n",
    "    ratings = np.random.choice([1, 2, 3, 4, 5], n_ratings, \n",
    "                              p=[0.05, 0.1, 0.2, 0.35, 0.3])\n",
    "    \n",
    "    # Add timestamps (random but sorted per user)\n",
    "    timestamps = []\n",
    "    for user_id in user_ids:\n",
    "        base_time = 1000000000 + np.random.randint(0, 500000000)\n",
    "        timestamps.append(base_time)\n",
    "    \n",
    "    sample_df = pd.DataFrame({\n",
    "        'userId': user_ids,\n",
    "        'movieId': movie_ids,\n",
    "        'rating': ratings,\n",
    "        'timestamp': timestamps\n",
    "    })\n",
    "    \n",
    "    # Remove duplicates\n",
    "    sample_df = sample_df.drop_duplicates(subset=['userId', 'movieId'])\n",
    "    \n",
    "    print(f\"ðŸŽ² Created sample dataset with {len(sample_df):,} ratings\")\n",
    "    return sample_df\n",
    "\n",
    "def create_sample_movies(movie_ids):\n",
    "    \"\"\"Create sample movies data.\"\"\"\n",
    "    genres = ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', \n",
    "             'Documentary', 'Drama', 'Fantasy', 'Horror', 'Musical', 'Mystery', \n",
    "             'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n",
    "    \n",
    "    movies_data = []\n",
    "    for movie_id in movie_ids:\n",
    "        # Random title and year\n",
    "        year = np.random.randint(1950, 2023)\n",
    "        title = f\"Movie {movie_id} ({year})\"\n",
    "        \n",
    "        # Random genres (1-3 genres per movie)\n",
    "        n_genres = np.random.randint(1, 4)\n",
    "        movie_genres = np.random.choice(genres, n_genres, replace=False)\n",
    "        \n",
    "        movies_data.append({\n",
    "            'movieId': movie_id,\n",
    "            'title': title,\n",
    "            'genres': '|'.join(movie_genres)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(movies_data)\n",
    "\n",
    "# Load the data\n",
    "print(\"ðŸ“Š Loading MovieLens dataset...\")\n",
    "ratings_df, movies_df = load_movielens_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b5f134",
   "metadata": {},
   "source": [
    "## 2. Data Exploration and Preprocessing ðŸ”\n",
    "\n",
    "Let's explore the dataset structure, analyze rating distributions, and understand user and movie behavior patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c693e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"ðŸ“ˆ Dataset Overview\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ðŸ“Š Ratings DataFrame: {ratings_df.shape}\")\n",
    "print(f\"ðŸŽ¬ Movies DataFrame: {movies_df.shape}\")\n",
    "print(f\"ðŸ‘¥ Unique Users: {ratings_df['userId'].nunique():,}\")\n",
    "print(f\"ðŸŽ­ Unique Movies: {ratings_df['movieId'].nunique():,}\")\n",
    "print(f\"â­ Total Ratings: {len(ratings_df):,}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nðŸ” First 5 ratings:\")\n",
    "display(ratings_df.head())\n",
    "\n",
    "print(\"\\nðŸŽ¬ First 5 movies:\")\n",
    "display(movies_df.head())\n",
    "\n",
    "# Data types and missing values\n",
    "print(\"\\nðŸ“‹ Ratings Data Info:\")\n",
    "print(ratings_df.info())\n",
    "\n",
    "print(\"\\nðŸ“‹ Movies Data Info:\")  \n",
    "print(movies_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d425c269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating Distribution Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Rating Distribution\n",
    "ratings_df['rating'].hist(bins=10, ax=axes[0, 0], color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Rating Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Rating')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# Add statistics\n",
    "mean_rating = ratings_df['rating'].mean()\n",
    "std_rating = ratings_df['rating'].std()\n",
    "axes[0, 0].axvline(mean_rating, color='red', linestyle='--', \n",
    "                   label=f'Mean: {mean_rating:.2f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. User Activity Distribution\n",
    "user_activity = ratings_df.groupby('userId').size()\n",
    "user_activity.hist(bins=50, ax=axes[0, 1], color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_title('User Activity Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Number of Ratings per User')\n",
    "axes[0, 1].set_ylabel('Number of Users')\n",
    "axes[0, 1].set_yscale('log')\n",
    "\n",
    "# 3. Movie Popularity Distribution\n",
    "movie_popularity = ratings_df.groupby('movieId').size()\n",
    "movie_popularity.hist(bins=50, ax=axes[1, 0], color='orange', edgecolor='black')\n",
    "axes[1, 0].set_title('Movie Popularity Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Number of Ratings per Movie')\n",
    "axes[1, 0].set_ylabel('Number of Movies')\n",
    "axes[1, 0].set_yscale('log')\n",
    "\n",
    "# 4. Average Rating per Movie\n",
    "movie_avg_ratings = ratings_df.groupby('movieId')['rating'].mean()\n",
    "movie_avg_ratings.hist(bins=30, ax=axes[1, 1], color='purple', edgecolor='black')\n",
    "axes[1, 1].set_title('Average Rating per Movie', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Average Rating')\n",
    "axes[1, 1].set_ylabel('Number of Movies')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistical Summary\n",
    "print(\"\\nðŸ“Š Statistical Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"â­ Average Rating: {mean_rating:.3f} Â± {std_rating:.3f}\")\n",
    "print(f\"ðŸ“ˆ Rating Range: {ratings_df['rating'].min()} - {ratings_df['rating'].max()}\")\n",
    "print(f\"ðŸ‘¥ Avg Ratings per User: {user_activity.mean():.1f}\")\n",
    "print(f\"ðŸ“½ï¸ Avg Ratings per Movie: {movie_popularity.mean():.1f}\")\n",
    "print(f\"ðŸŽ¯ Most Active User: {user_activity.max()} ratings\")\n",
    "print(f\"ðŸŒŸ Most Popular Movie: {movie_popularity.max()} ratings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b4b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Sparsity Analysis\n",
    "def analyze_sparsity(ratings_df):\n",
    "    \"\"\"Analyze dataset sparsity.\"\"\"\n",
    "    n_users = ratings_df['userId'].nunique()\n",
    "    n_movies = ratings_df['movieId'].nunique()\n",
    "    n_ratings = len(ratings_df)\n",
    "    \n",
    "    # Total possible ratings\n",
    "    total_possible = n_users * n_movies\n",
    "    sparsity = 1 - (n_ratings / total_possible)\n",
    "    \n",
    "    print(\"ðŸ•³ï¸ Sparsity Analysis\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"Users: {n_users:,}\")\n",
    "    print(f\"Movies: {n_movies:,}\")\n",
    "    print(f\"Actual Ratings: {n_ratings:,}\")\n",
    "    print(f\"Possible Ratings: {total_possible:,}\")\n",
    "    print(f\"Sparsity: {sparsity:.4f} ({sparsity*100:.2f}%)\")\n",
    "    \n",
    "    return sparsity\n",
    "\n",
    "sparsity = analyze_sparsity(ratings_df)\n",
    "\n",
    "# Data Preprocessing\n",
    "print(\"\\nðŸ”§ Data Preprocessing\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Merge ratings with movie information\n",
    "merged_df = ratings_df.merge(movies_df, on='movieId', how='left')\n",
    "print(f\"âœ… Merged dataset: {merged_df.shape}\")\n",
    "\n",
    "# Filter users and movies with minimum number of ratings\n",
    "MIN_USER_RATINGS = 20\n",
    "MIN_MOVIE_RATINGS = 10\n",
    "\n",
    "print(f\"\\nðŸ“Š Applying filters:\")\n",
    "print(f\"   Minimum ratings per user: {MIN_USER_RATINGS}\")\n",
    "print(f\"   Minimum ratings per movie: {MIN_MOVIE_RATINGS}\")\n",
    "\n",
    "# Count ratings per user and movie\n",
    "user_counts = merged_df['userId'].value_counts()\n",
    "movie_counts = merged_df['movieId'].value_counts()\n",
    "\n",
    "# Filter active users and popular movies\n",
    "active_users = user_counts[user_counts >= MIN_USER_RATINGS].index\n",
    "popular_movies = movie_counts[movie_counts >= MIN_MOVIE_RATINGS].index\n",
    "\n",
    "# Apply filters\n",
    "filtered_df = merged_df[\n",
    "    (merged_df['userId'].isin(active_users)) &\n",
    "    (merged_df['movieId'].isin(popular_movies))\n",
    "]\n",
    "\n",
    "print(f\"ðŸ“‰ After filtering:\")\n",
    "print(f\"   Users: {merged_df['userId'].nunique():,} â†’ {filtered_df['userId'].nunique():,}\")\n",
    "print(f\"   Movies: {merged_df['movieId'].nunique():,} â†’ {filtered_df['movieId'].nunique():,}\")\n",
    "print(f\"   Ratings: {len(merged_df):,} â†’ {len(filtered_df):,}\")\n",
    "\n",
    "# Analyze new sparsity\n",
    "print(f\"\\nðŸŽ¯ New sparsity:\")\n",
    "new_sparsity = analyze_sparsity(filtered_df)\n",
    "\n",
    "# Save processed data\n",
    "filtered_df.to_csv(PROCESSED_DATA_PATH / \"filtered_ratings.csv\", index=False)\n",
    "print(f\"\\nðŸ’¾ Saved filtered dataset to {PROCESSED_DATA_PATH / 'filtered_ratings.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3accf5b9",
   "metadata": {},
   "source": [
    "## 3. Movie-Movie Similarity Matrix ðŸŽ¬âž¡ï¸ðŸŽ¬\n",
    "\n",
    "Let's create a movie-movie similarity matrix using cosine similarity. This will help us find movies that are similar based on user rating patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ce431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create User-Item Matrix\n",
    "def create_user_item_matrix(df):\n",
    "    \"\"\"Create a user-item matrix from ratings data.\"\"\"\n",
    "    user_item_matrix = df.pivot_table(\n",
    "        index='userId', \n",
    "        columns='movieId', \n",
    "        values='rating',\n",
    "        fill_value=0\n",
    "    )\n",
    "    return user_item_matrix\n",
    "\n",
    "print(\"ðŸ”§ Creating User-Item Matrix...\")\n",
    "user_item_matrix = create_user_item_matrix(filtered_df)\n",
    "print(f\"âœ… User-Item Matrix shape: {user_item_matrix.shape}\")\n",
    "\n",
    "# Create Movie-Movie Similarity Matrix\n",
    "def calculate_movie_similarity(user_item_matrix, method='cosine'):\n",
    "    \"\"\"Calculate movie-to-movie similarity matrix.\"\"\"\n",
    "    print(f\"ðŸ”„ Calculating movie similarity using {method} method...\")\n",
    "    \n",
    "    # Transpose to get movies as rows\n",
    "    movie_matrix = user_item_matrix.T\n",
    "    \n",
    "    if method == 'cosine':\n",
    "        # Replace 0s with NaN for better similarity calculation\n",
    "        movie_matrix_filled = movie_matrix.replace(0, np.nan)\n",
    "        \n",
    "        # Fill NaN with 0 for cosine similarity\n",
    "        matrix_for_sim = movie_matrix_filled.fillna(0).values\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity_matrix = cosine_similarity(matrix_for_sim)\n",
    "        \n",
    "    elif method == 'pearson':\n",
    "        # Calculate Pearson correlation\n",
    "        similarity_matrix = movie_matrix.T.corr().fillna(0).values\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    similarity_df = pd.DataFrame(\n",
    "        similarity_matrix,\n",
    "        index=movie_matrix.index,\n",
    "        columns=movie_matrix.index\n",
    "    )\n",
    "    \n",
    "    return similarity_df\n",
    "\n",
    "# Calculate similarity matrix\n",
    "movie_similarity_df = calculate_movie_similarity(user_item_matrix, method='cosine')\n",
    "print(f\"âœ… Movie similarity matrix: {movie_similarity_df.shape}\")\n",
    "\n",
    "# Visualize similarity matrix (sample)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sample_movies = movie_similarity_df.index[:50]  # Sample first 50 movies\n",
    "sample_sim_matrix = movie_similarity_df.loc[sample_movies, sample_movies]\n",
    "\n",
    "sns.heatmap(sample_sim_matrix, \n",
    "            cmap='RdYlBu_r', \n",
    "            center=0, \n",
    "            square=True,\n",
    "            cbar_kws={'label': 'Cosine Similarity'})\n",
    "plt.title('Movie-Movie Similarity Matrix (Sample)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Movie ID')\n",
    "plt.ylabel('Movie ID')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ðŸŽ¯ Similarity matrix statistics:\")\n",
    "print(f\"   Mean similarity: {movie_similarity_df.values.mean():.4f}\")\n",
    "print(f\"   Std similarity: {movie_similarity_df.values.std():.4f}\")\n",
    "print(f\"   Max similarity: {movie_similarity_df.values.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7071811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find similar movies\n",
    "def get_similar_movies(movie_id, similarity_matrix, movies_df, n_similar=10):\n",
    "    \"\"\"Find movies most similar to a given movie.\"\"\"\n",
    "    if movie_id not in similarity_matrix.index:\n",
    "        return f\"Movie ID {movie_id} not found in similarity matrix\"\n",
    "    \n",
    "    # Get similarities for the target movie\n",
    "    similarities = similarity_matrix.loc[movie_id]\n",
    "    \n",
    "    # Sort and get top N (excluding the movie itself)\n",
    "    similar_movies = similarities.sort_values(ascending=False)[1:n_similar+1]\n",
    "    \n",
    "    # Get movie information\n",
    "    results = []\n",
    "    for sim_movie_id, similarity in similar_movies.items():\n",
    "        movie_info = movies_df[movies_df['movieId'] == sim_movie_id]\n",
    "        if len(movie_info) > 0:\n",
    "            title = movie_info.iloc[0]['title']\n",
    "            genres = movie_info.iloc[0]['genres']\n",
    "            results.append({\n",
    "                'movieId': sim_movie_id,\n",
    "                'title': title,\n",
    "                'genres': genres,\n",
    "                'similarity': similarity\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example: Find movies similar to a popular movie\n",
    "sample_movie_id = filtered_df['movieId'].value_counts().index[0]  # Most popular movie\n",
    "sample_movie_info = movies_df[movies_df['movieId'] == sample_movie_id].iloc[0]\n",
    "\n",
    "print(f\"ðŸŽ¬ Finding movies similar to:\")\n",
    "print(f\"   ID: {sample_movie_id}\")\n",
    "print(f\"   Title: {sample_movie_info['title']}\")\n",
    "print(f\"   Genres: {sample_movie_info['genres']}\")\n",
    "\n",
    "similar_movies = get_similar_movies(sample_movie_id, movie_similarity_df, movies_df, n_similar=10)\n",
    "print(f\"\\nðŸŽ¯ Top 10 Similar Movies:\")\n",
    "display(similar_movies)\n",
    "\n",
    "# Analyze similarity distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "similarity_values = movie_similarity_df.values\n",
    "similarity_values = similarity_values[similarity_values != 1.0]  # Remove self-similarity\n",
    "plt.hist(similarity_values, bins=50, color='lightblue', edgecolor='black', alpha=0.7)\n",
    "plt.title('Distribution of Movie Similarities', fontweight='bold')\n",
    "plt.xlabel('Cosine Similarity')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# Box plot of similarities for a sample movie\n",
    "sample_similarities = movie_similarity_df.iloc[0, 1:]  # First movie's similarities\n",
    "plt.boxplot(sample_similarities, vert=True)\n",
    "plt.title('Similarity Distribution for One Movie', fontweight='bold')\n",
    "plt.ylabel('Cosine Similarity')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7957f35",
   "metadata": {},
   "source": [
    "## 4. User-User Similarity Matrix ðŸ‘¥âž¡ï¸ðŸ‘¥\n",
    "\n",
    "Now let's create a user-user similarity matrix to identify users with similar preferences and tastes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd3955e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create User-User Similarity Matrix\n",
    "def calculate_user_similarity(user_item_matrix, method='cosine', sample_size=1000):\n",
    "    \"\"\"Calculate user-to-user similarity matrix (with sampling for efficiency).\"\"\"\n",
    "    print(f\"ðŸ”„ Calculating user similarity using {method} method...\")\n",
    "    \n",
    "    # Sample users for computational efficiency\n",
    "    if len(user_item_matrix) > sample_size:\n",
    "        print(f\"ðŸ“Š Sampling {sample_size} users for efficiency...\")\n",
    "        sampled_users = np.random.choice(user_item_matrix.index, sample_size, replace=False)\n",
    "        user_matrix_sample = user_item_matrix.loc[sampled_users]\n",
    "    else:\n",
    "        user_matrix_sample = user_item_matrix\n",
    "    \n",
    "    if method == 'cosine':\n",
    "        # Replace 0s with NaN, then back to 0 for cosine similarity\n",
    "        matrix_filled = user_matrix_sample.replace(0, np.nan).fillna(0).values\n",
    "        similarity_matrix = cosine_similarity(matrix_filled)\n",
    "    \n",
    "    elif method == 'pearson':\n",
    "        # Calculate Pearson correlation\n",
    "        similarity_matrix = user_matrix_sample.T.corr().fillna(0).values\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    similarity_df = pd.DataFrame(\n",
    "        similarity_matrix,\n",
    "        index=user_matrix_sample.index,\n",
    "        columns=user_matrix_sample.index\n",
    "    )\n",
    "    \n",
    "    return similarity_df\n",
    "\n",
    "# Calculate user similarity matrix (with sampling for efficiency)\n",
    "user_similarity_df = calculate_user_similarity(user_item_matrix, method='cosine', sample_size=500)\n",
    "print(f\"âœ… User similarity matrix: {user_similarity_df.shape}\")\n",
    "\n",
    "# Visualize user similarity matrix (sample)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sample_users = user_similarity_df.index[:50]  # Sample first 50 users\n",
    "sample_user_sim = user_similarity_df.loc[sample_users, sample_users]\n",
    "\n",
    "sns.heatmap(sample_user_sim, \n",
    "            cmap='RdYlBu_r', \n",
    "            center=0, \n",
    "            square=True,\n",
    "            cbar_kws={'label': 'Cosine Similarity'})\n",
    "plt.title('User-User Similarity Matrix (Sample)', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('User ID')\n",
    "plt.ylabel('User ID')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"ðŸŽ¯ User similarity statistics:\")\n",
    "print(f\"   Mean similarity: {user_similarity_df.values.mean():.4f}\")\n",
    "print(f\"   Std similarity: {user_similarity_df.values.std():.4f}\")\n",
    "print(f\"   Max similarity: {user_similarity_df.values.max():.4f}\")\n",
    "\n",
    "# Function to find similar users\n",
    "def get_similar_users(user_id, similarity_matrix, user_item_matrix, n_similar=10):\n",
    "    \"\"\"Find users most similar to a given user.\"\"\"\n",
    "    if user_id not in similarity_matrix.index:\n",
    "        return f\"User ID {user_id} not found in similarity matrix\"\n",
    "    \n",
    "    # Get similarities for the target user\n",
    "    similarities = similarity_matrix.loc[user_id]\n",
    "    \n",
    "    # Sort and get top N (excluding the user themselves)\n",
    "    similar_users = similarities.sort_values(ascending=False)[1:n_similar+1]\n",
    "    \n",
    "    # Get user statistics\n",
    "    results = []\n",
    "    for sim_user_id, similarity in similar_users.items():\n",
    "        user_ratings = user_item_matrix.loc[sim_user_id]\n",
    "        n_ratings = (user_ratings > 0).sum()\n",
    "        avg_rating = user_ratings[user_ratings > 0].mean()\n",
    "        \n",
    "        results.append({\n",
    "            'userId': sim_user_id,\n",
    "            'similarity': similarity,\n",
    "            'num_ratings': n_ratings,\n",
    "            'avg_rating': avg_rating\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example: Find similar users\n",
    "sample_user_id = user_similarity_df.index[0]\n",
    "print(f\"\\nðŸ‘¤ Finding users similar to User ID: {sample_user_id}\")\n",
    "\n",
    "# Get sample user's stats\n",
    "sample_user_ratings = user_item_matrix.loc[sample_user_id]\n",
    "sample_user_n_ratings = (sample_user_ratings > 0).sum()\n",
    "sample_user_avg = sample_user_ratings[sample_user_ratings > 0].mean()\n",
    "\n",
    "print(f\"   Number of ratings: {sample_user_n_ratings}\")\n",
    "print(f\"   Average rating: {sample_user_avg:.2f}\")\n",
    "\n",
    "similar_users = get_similar_users(sample_user_id, user_similarity_df, user_item_matrix, n_similar=10)\n",
    "print(f\"\\nðŸŽ¯ Top 10 Similar Users:\")\n",
    "display(similar_users)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbac53f",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering for ML Models ðŸ› ï¸\n",
    "\n",
    "Let's create intelligent features that will help our machine learning models predict ratings accurately. These features capture user behavior, movie characteristics, and interaction patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f203c45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "def create_ml_features(ratings_df):\n",
    "    \"\"\"Create comprehensive features for machine learning models.\"\"\"\n",
    "    print(\"ðŸ”§ Creating ML features...\")\n",
    "    \n",
    "    # Start with the ratings data\n",
    "    features_df = ratings_df.copy()\n",
    "    \n",
    "    # 1. Global Statistics\n",
    "    global_mean = ratings_df['rating'].mean()\n",
    "    features_df['rating_gmean'] = global_mean\n",
    "    \n",
    "    print(f\"   âœ… Global mean rating: {global_mean:.3f}\")\n",
    "    \n",
    "    # 2. User-based Features\n",
    "    user_stats = ratings_df.groupby('userId')['rating'].agg([\n",
    "        'mean', 'std', 'count', 'min', 'max'\n",
    "    ]).add_prefix('user_')\n",
    "    \n",
    "    # User bias (difference from global mean)\n",
    "    user_stats['user_bias'] = user_stats['user_mean'] - global_mean\n",
    "    \n",
    "    # Fill NaN std with 0 (users with only one rating)\n",
    "    user_stats['user_std'] = user_stats['user_std'].fillna(0)\n",
    "    \n",
    "    # Merge user features\n",
    "    features_df = features_df.merge(user_stats, left_on='userId', right_index=True, how='left')\n",
    "    \n",
    "    print(f\"   âœ… User features: {list(user_stats.columns)}\")\n",
    "    \n",
    "    # 3. Movie-based Features  \n",
    "    movie_stats = ratings_df.groupby('movieId')['rating'].agg([\n",
    "        'mean', 'std', 'count', 'min', 'max'\n",
    "    ]).add_prefix('movie_')\n",
    "    \n",
    "    # Movie bias (difference from global mean)\n",
    "    movie_stats['movie_bias'] = movie_stats['movie_mean'] - global_mean\n",
    "    \n",
    "    # Fill NaN std with 0 (movies with only one rating)\n",
    "    movie_stats['movie_std'] = movie_stats['movie_std'].fillna(0)\n",
    "    \n",
    "    # Merge movie features\n",
    "    features_df = features_df.merge(movie_stats, left_on='movieId', right_index=True, how='left')\n",
    "    \n",
    "    print(f\"   âœ… Movie features: {list(movie_stats.columns)}\")\n",
    "    \n",
    "    # 4. Advanced Features\n",
    "    \n",
    "    # User activity levels\n",
    "    features_df['user_activity_percentile'] = features_df['user_count'].rank(pct=True)\n",
    "    features_df['is_heavy_user'] = (features_df['user_count'] > features_df['user_count'].quantile(0.8)).astype(int)\n",
    "    features_df['is_light_user'] = (features_df['user_count'] < features_df['user_count'].quantile(0.2)).astype(int)\n",
    "    \n",
    "    # Movie popularity levels\n",
    "    features_df['movie_popularity_percentile'] = features_df['movie_count'].rank(pct=True)\n",
    "    features_df['is_popular_movie'] = (features_df['movie_count'] > features_df['movie_count'].quantile(0.8)).astype(int)\n",
    "    features_df['is_niche_movie'] = (features_df['movie_count'] < features_df['movie_count'].quantile(0.2)).astype(int)\n",
    "    \n",
    "    # Rating ranges\n",
    "    features_df['user_rating_range'] = features_df['user_max'] - features_df['user_min']\n",
    "    features_df['movie_rating_range'] = features_df['movie_max'] - features_df['movie_min']\n",
    "    \n",
    "    # Z-scores (how unusual is this rating)\n",
    "    features_df['user_rating_zscore'] = (features_df['rating'] - features_df['user_mean']) / (features_df['user_std'] + 1e-8)\n",
    "    features_df['movie_rating_zscore'] = (features_df['rating'] - features_df['movie_mean']) / (features_df['movie_std'] + 1e-8)\n",
    "    \n",
    "    # Interaction features\n",
    "    features_df['user_movie_bias_interaction'] = features_df['user_bias'] * features_df['movie_bias']\n",
    "    features_df['user_activity_movie_popularity'] = features_df['user_count'] * features_df['movie_count']\n",
    "    \n",
    "    # Deviation features\n",
    "    features_df['rating_deviation_from_user_mean'] = features_df['rating'] - features_df['user_mean']\n",
    "    features_df['rating_deviation_from_movie_mean'] = features_df['rating'] - features_df['movie_mean']\n",
    "    features_df['rating_deviation_from_global_mean'] = features_df['rating'] - features_df['rating_gmean']\n",
    "    \n",
    "    print(f\"   âœ… Advanced features created\")\n",
    "    print(f\"   ðŸ“Š Total features: {features_df.shape[1]}\")\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# Create features\n",
    "features_df = create_ml_features(filtered_df)\n",
    "\n",
    "# Display feature information\n",
    "print(f\"\\nðŸ“‹ Feature DataFrame Shape: {features_df.shape}\")\n",
    "print(f\"\\nðŸ” Feature Columns:\")\n",
    "feature_cols = [col for col in features_df.columns if col not in ['userId', 'movieId', 'rating', 'timestamp', 'title', 'genres']]\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"   {i:2d}. {col}\")\n",
    "\n",
    "# Feature correlation analysis\n",
    "print(f\"\\nðŸ“Š Feature Importance Analysis\")\n",
    "\n",
    "# Select only numeric features for correlation\n",
    "numeric_features = features_df.select_dtypes(include=[np.number])\n",
    "feature_columns = [col for col in numeric_features.columns \n",
    "                  if col not in ['userId', 'movieId', 'rating', 'timestamp']]\n",
    "\n",
    "# Calculate correlations with target (rating)\n",
    "correlations = {}\n",
    "for col in feature_columns:\n",
    "    if numeric_features[col].std() > 0:  # Avoid constant features\n",
    "        corr = numeric_features[col].corr(numeric_features['rating'])\n",
    "        if not np.isnan(corr):\n",
    "            correlations[col] = abs(corr)\n",
    "\n",
    "# Sort by correlation strength\n",
    "sorted_correlations = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Top 15 Most Important Features (by correlation with rating):\")\n",
    "for i, (feature, correlation) in enumerate(sorted_correlations[:15], 1):\n",
    "    print(f\"   {i:2d}. {feature}: {correlation:.4f}\")\n",
    "\n",
    "# Visualize top features\n",
    "top_features = [item[0] for item in sorted_correlations[:10]]\n",
    "top_correlations = [item[1] for item in sorted_correlations[:10]]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(top_features)), top_correlations, color='skyblue')\n",
    "plt.yticks(range(len(top_features)), top_features)\n",
    "plt.xlabel('Absolute Correlation with Rating')\n",
    "plt.title('Top 10 Most Important Features', fontsize=16, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save features for ML training\n",
    "features_df.to_csv(PROCESSED_DATA_PATH / \"ml_features.csv\", index=False)\n",
    "print(f\"\\nðŸ’¾ Features saved to {PROCESSED_DATA_PATH / 'ml_features.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9dc536",
   "metadata": {},
   "source": [
    "## 6. Train Machine Learning Models ðŸ¤–\n",
    "\n",
    "Now let's train multiple machine learning models to predict movie ratings using our engineered features. We'll compare different algorithms to find the best performer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7faa4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for ML training\n",
    "def prepare_ml_data(features_df):\n",
    "    \"\"\"Prepare features and target for ML training.\"\"\"\n",
    "    # Select feature columns (exclude IDs, target, and metadata)\n",
    "    exclude_cols = ['userId', 'movieId', 'rating', 'timestamp', 'title', 'genres']\n",
    "    \n",
    "    # Select numeric features only\n",
    "    numeric_df = features_df.select_dtypes(include=[np.number])\n",
    "    feature_cols = [col for col in numeric_df.columns if col not in exclude_cols]\n",
    "    \n",
    "    # Features and target\n",
    "    X = numeric_df[feature_cols].fillna(0)\n",
    "    y = numeric_df['rating']\n",
    "    \n",
    "    # Remove rows with missing target\n",
    "    valid_mask = ~y.isna()\n",
    "    X = X[valid_mask]\n",
    "    y = y[valid_mask]\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "print(\"ðŸ”§ Preparing data for ML training...\")\n",
    "X, y, feature_cols = prepare_ml_data(features_df)\n",
    "print(f\"âœ… Prepared {X.shape[0]} samples with {X.shape[1]} features\")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=None\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“Š Data split:\")\n",
    "print(f\"   Training: {X_train.shape[0]} samples\")\n",
    "print(f\"   Testing: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Scale features for linear models\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define evaluation metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \\\"\\\"\\\"Calculate regression metrics.\\\"\\\"\\\"\\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\\n    mae = mean_absolute_error(y_true, y_pred)\\n    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\\n    \\n    return {\\n        'RMSE': rmse,\\n        'MAE': mae,\\n        'MAPE': mape\\n    }\\n\\n# Train multiple models\\nprint(f\\\"\\\\nðŸ¤– Training ML Models...\\\")\\nmodels = {}\\nresults = {}\\n\\n# 1. Linear Regression\\nprint(\\\"   ðŸ“ˆ Training Linear Regression...\\\")\\nlr_model = LinearRegression()\\nlr_model.fit(X_train_scaled, y_train)\\n\\ny_pred_train_lr = lr_model.predict(X_train_scaled)\\ny_pred_test_lr = lr_model.predict(X_test_scaled)\\n\\nmodels['Linear Regression'] = lr_model\\nresults['Linear Regression'] = {\\n    'Train': calculate_metrics(y_train, y_pred_train_lr),\\n    'Test': calculate_metrics(y_test, y_pred_test_lr)\\n}\\n\\n# 2. Ridge Regression\\nprint(\\\"   ðŸ”ï¸ Training Ridge Regression...\\\")\\nridge_model = Ridge(alpha=1.0)\\nridge_model.fit(X_train_scaled, y_train)\\n\\ny_pred_train_ridge = ridge_model.predict(X_train_scaled)\\ny_pred_test_ridge = ridge_model.predict(X_test_scaled)\\n\\nmodels['Ridge Regression'] = ridge_model\\nresults['Ridge Regression'] = {\\n    'Train': calculate_metrics(y_train, y_pred_train_ridge),\\n    'Test': calculate_metrics(y_test, y_pred_test_ridge)\\n}\\n\\n# 3. Random Forest\\nprint(\\\"   ðŸŒ³ Training Random Forest...\\\")\\nrf_model = RandomForestRegressor(\\n    n_estimators=100, \\n    max_depth=10, \\n    random_state=42,\\n    n_jobs=-1\\n)\\nrf_model.fit(X_train, y_train)\\n\\ny_pred_train_rf = rf_model.predict(X_train)\\ny_pred_test_rf = rf_model.predict(X_test)\\n\\nmodels['Random Forest'] = rf_model\\nresults['Random Forest'] = {\\n    'Train': calculate_metrics(y_train, y_pred_train_rf),\\n    'Test': calculate_metrics(y_test, y_pred_test_rf)\\n}\\n\\n# 4. Gradient Boosting\\nprint(\\\"   ðŸš€ Training Gradient Boosting...\\\")\\ngb_model = GradientBoostingRegressor(\\n    n_estimators=100,\\n    max_depth=6,\\n    learning_rate=0.1,\\n    random_state=42\\n)\\ngb_model.fit(X_train, y_train)\\n\\ny_pred_train_gb = gb_model.predict(X_train)\\ny_pred_test_gb = gb_model.predict(X_test)\\n\\nmodels['Gradient Boosting'] = gb_model\\nresults['Gradient Boosting'] = {\\n    'Train': calculate_metrics(y_train, y_pred_train_gb),\\n    'Test': calculate_metrics(y_test, y_pred_test_gb)\\n}\\n\\n# 5. XGBoost (if available)\\ntry:\\n    print(\\\"   âš¡ Training XGBoost...\\\")\\n    xgb_model = xgb.XGBRegressor(\\n        n_estimators=200,\\n        max_depth=8,\\n        learning_rate=0.1,\\n        subsample=0.9,\\n        random_state=42\\n    )\\n    xgb_model.fit(X_train, y_train)\\n    \\n    y_pred_train_xgb = xgb_model.predict(X_train)\\n    y_pred_test_xgb = xgb_model.predict(X_test)\\n    \\n    models['XGBoost'] = xgb_model\\n    results['XGBoost'] = {\\n        'Train': calculate_metrics(y_train, y_pred_train_xgb),\\n        'Test': calculate_metrics(y_test, y_pred_test_xgb)\\n    }\\nexcept NameError:\\n    print(\\\"   âš ï¸ XGBoost not available\\\")\\n\\n# 6. LightGBM (if available)\\ntry:\\n    print(\\\"   ðŸ’¡ Training LightGBM...\\\")\\n    lgb_model = lgb.LGBMRegressor(\\n        n_estimators=200,\\n        max_depth=8,\\n        learning_rate=0.1,\\n        subsample=0.9,\\n        random_state=42,\\n        verbose=-1\\n    )\\n    lgb_model.fit(X_train, y_train)\\n    \\n    y_pred_train_lgb = lgb_model.predict(X_train)\\n    y_pred_test_lgb = lgb_model.predict(X_test)\\n    \\n    models['LightGBM'] = lgb_model\\n    results['LightGBM'] = {\\n        'Train': calculate_metrics(y_train, y_pred_train_lgb),\\n        'Test': calculate_metrics(y_test, y_pred_test_lgb)\\n    }\\nexcept NameError:\\n    print(\\\"   âš ï¸ LightGBM not available\\\")\\n\\nprint(f\\\"\\\\nâœ… Trained {len(models)} models successfully!\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468dfd70",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation and Comparison ðŸ“Š\n",
    "\n",
    "Let's evaluate all our models using RMSE and MAPE metrics to identify the best performer for movie rating prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbad570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model comparison DataFrame\n",
    "def create_comparison_table(results):\n",
    "    \\\"\\\"\\\"Create a comparison table of model results.\\\"\\\"\\\"\\n    comparison_data = []\\n    \\n    for model_name, metrics in results.items():\\n        train_metrics = metrics['Train']\\n        test_metrics = metrics['Test']\\n        \\n        comparison_data.append({\\n            'Model': model_name,\\n            'Train_RMSE': train_metrics['RMSE'],\\n            'Test_RMSE': test_metrics['RMSE'],\\n            'Train_MAE': train_metrics['MAE'],\\n            'Test_MAE': test_metrics['MAE'],\\n            'Train_MAPE': train_metrics['MAPE'],\\n            'Test_MAPE': test_metrics['MAPE']\\n        })\\n    \\n    comparison_df = pd.DataFrame(comparison_data)\\n    comparison_df = comparison_df.round(4)\\n    \\n    # Sort by test RMSE (lower is better)\\n    comparison_df = comparison_df.sort_values('Test_RMSE')\\n    \\n    return comparison_df\\n\\n# Create and display comparison table\\ncomparison_df = create_comparison_table(results)\\nprint(\\\"ðŸ† Model Performance Comparison (sorted by Test RMSE)\\\")\\nprint(\\\"=\\\" * 70)\\ndisplay(comparison_df)\\n\\n# Identify best model\\nbest_model_name = comparison_df.iloc[0]['Model']\\nbest_rmse = comparison_df.iloc[0]['Test_RMSE']\\nbest_mape = comparison_df.iloc[0]['Test_MAPE']\\n\\nprint(f\\\"\\\\nðŸ¥‡ Best Model: {best_model_name}\\\")\\nprint(f\\\"   ðŸ“ˆ Test RMSE: {best_rmse:.4f}\\\")\\nprint(f\\\"   ðŸ“Š Test MAPE: {best_mape:.2f}%\\\")\\n\\n# Visualize model comparison\\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\\n\\n# RMSE Comparison\\naxes[0].bar(comparison_df['Model'], comparison_df['Test_RMSE'], color='lightcoral', alpha=0.7)\\naxes[0].set_title('Test RMSE Comparison', fontweight='bold')\\naxes[0].set_ylabel('RMSE')\\naxes[0].tick_params(axis='x', rotation=45)\\n\\n# MAE Comparison\\naxes[1].bar(comparison_df['Model'], comparison_df['Test_MAE'], color='lightblue', alpha=0.7)\\naxes[1].set_title('Test MAE Comparison', fontweight='bold')\\naxes[1].set_ylabel('MAE')\\naxes[1].tick_params(axis='x', rotation=45)\\n\\n# MAPE Comparison\\naxes[2].bar(comparison_df['Model'], comparison_df['Test_MAPE'], color='lightgreen', alpha=0.7)\\naxes[2].set_title('Test MAPE Comparison', fontweight='bold')\\naxes[2].set_ylabel('MAPE (%)')\\naxes[2].tick_params(axis='x', rotation=45)\\n\\nplt.tight_layout()\\nplt.show()\\n\\n# Feature importance for best tree-based model\\nif best_model_name in ['Random Forest', 'Gradient Boosting', 'XGBoost', 'LightGBM']:\\n    best_model = models[best_model_name]\\n    \\n    if hasattr(best_model, 'feature_importances_'):\\n        feature_importance = pd.DataFrame({\\n            'feature': feature_cols,\\n            'importance': best_model.feature_importances_\\n        }).sort_values('importance', ascending=False)\\n        \\n        print(f\\\"\\\\nðŸŽ¯ Top 15 Feature Importance ({best_model_name}):\\\")\\n        display(feature_importance.head(15))\\n        \\n        # Plot feature importance\\n        plt.figure(figsize=(12, 8))\\n        top_features = feature_importance.head(15)\\n        plt.barh(range(len(top_features)), top_features['importance'], color='skyblue')\\n        plt.yticks(range(len(top_features)), top_features['feature'])\\n        plt.xlabel('Feature Importance')\\n        plt.title(f'Top 15 Feature Importance - {best_model_name}', fontsize=16, fontweight='bold')\\n        plt.gca().invert_yaxis()\\n        plt.tight_layout()\\n        plt.show()\\n\\n# Prediction vs Actual scatter plot for best model\\nif best_model_name in models:\\n    best_model = models[best_model_name]\\n    \\n    # Get predictions\\n    if best_model_name in ['Linear Regression', 'Ridge Regression']:\\n        y_pred_best = best_model.predict(X_test_scaled)\\n    else:\\n        y_pred_best = best_model.predict(X_test)\\n    \\n    # Create scatter plot\\n    plt.figure(figsize=(10, 8))\\n    plt.scatter(y_test, y_pred_best, alpha=0.5, color='blue')\\n    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\\n    plt.xlabel('Actual Rating')\\n    plt.ylabel('Predicted Rating')\\n    plt.title(f'Predicted vs Actual Ratings - {best_model_name}', fontsize=16, fontweight='bold')\\n    \\n    # Add statistics\\n    plt.text(0.05, 0.95, f'RMSE: {best_rmse:.4f}\\\\nMAPE: {best_mape:.2f}%', \\n             transform=plt.gca().transAxes, fontsize=12, \\n             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\\n    \\n    plt.tight_layout()\\n    plt.show()\\n\\n# Residual analysis\\nresiduals = y_test - y_pred_best\\n\\nplt.figure(figsize=(12, 5))\\n\\n# Residual histogram\\nplt.subplot(1, 2, 1)\\nplt.hist(residuals, bins=50, color='lightblue', edgecolor='black', alpha=0.7)\\nplt.xlabel('Residuals')\\nplt.ylabel('Frequency')\\nplt.title('Residual Distribution', fontweight='bold')\\nplt.axvline(0, color='red', linestyle='--')\\n\\n# Residual vs Predicted\\nplt.subplot(1, 2, 2)\\nplt.scatter(y_pred_best, residuals, alpha=0.5, color='green')\\nplt.xlabel('Predicted Rating')\\nplt.ylabel('Residuals')\\nplt.title('Residuals vs Predicted', fontweight='bold')\\nplt.axhline(0, color='red', linestyle='--')\\n\\nplt.tight_layout()\\nplt.show()\\n\\nprint(f\\\"\\\\nðŸ“Š Residual Analysis:\\\")\\nprint(f\\\"   Mean residual: {residuals.mean():.4f}\\\")\\nprint(f\\\"   Std residual: {residuals.std():.4f}\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e684caa0",
   "metadata": {},
   "source": [
    "## 8. Generate Movie Recommendations ðŸŽ¬âœ¨\n",
    "\n",
    "Finally, let's combine our similarity-based approach with ML predictions to create a comprehensive recommendation system that suggests movies to users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a043418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive recommendation system\\nclass MovieRecommendationSystem:\\n    def __init__(self, user_item_matrix, movie_similarity_df, models, scaler, \\n                 feature_cols, movies_df, features_df):\\n        self.user_item_matrix = user_item_matrix\\n        self.movie_similarity_df = movie_similarity_df\\n        self.models = models\\n        self.scaler = scaler\\n        self.feature_cols = feature_cols\\n        self.movies_df = movies_df\\n        self.features_df = features_df\\n        \\n        # Calculate statistics\\n        self.global_mean = features_df['rating'].mean()\\n        self.user_means = features_df.groupby('userId')['rating'].mean().to_dict()\\n        self.movie_means = features_df.groupby('movieId')['rating'].mean().to_dict()\\n    \\n    def get_user_rated_movies(self, user_id):\\n        \\\"\\\"\\\"Get movies already rated by user.\\\"\\\"\\\"\\n        user_ratings = self.user_item_matrix.loc[user_id] if user_id in self.user_item_matrix.index else pd.Series()\\n        return user_ratings[user_ratings > 0].index.tolist()\\n    \\n    def recommend_item_based(self, user_id, n_recommendations=10):\\n        \\\"\\\"\\\"Generate recommendations using item-based collaborative filtering.\\\"\\\"\\\"\\n        if user_id not in self.user_item_matrix.index:\\n            return self.get_popular_movies(n_recommendations)\\n        \\n        user_ratings = self.user_item_matrix.loc[user_id]\\n        rated_movies = user_ratings[user_ratings > 0]\\n        \\n        if len(rated_movies) == 0:\\n            return self.get_popular_movies(n_recommendations)\\n        \\n        # Calculate weighted scores for unrated movies\\n        movie_scores = {}\\n        \\n        for movie_id in self.movie_similarity_df.index:\\n            if movie_id not in rated_movies.index:  # Unrated movie\\n                score = 0\\n                weight_sum = 0\\n                \\n                for rated_movie, rating in rated_movies.items():\\n                    if rated_movie in self.movie_similarity_df.index:\\n                        similarity = self.movie_similarity_df.loc[movie_id, rated_movie]\\n                        if similarity > 0.1:  # Threshold\\n                            score += similarity * rating\\n                            weight_sum += similarity\\n                \\n                if weight_sum > 0:\\n                    movie_scores[movie_id] = score / weight_sum\\n        \\n        # Sort and return top recommendations\\n        sorted_movies = sorted(movie_scores.items(), key=lambda x: x[1], reverse=True)\\n        return [movie_id for movie_id, score in sorted_movies[:n_recommendations]]\\n    \\n    def recommend_ml_based(self, user_id, model_name='XGBoost', n_recommendations=10):\\n        \\\"\\\"\\\"Generate recommendations using ML model.\\\"\\\"\\\"\\n        if model_name not in self.models:\\n            return self.get_popular_movies(n_recommendations)\\n        \\n        model = self.models[model_name]\\n        rated_movies = self.get_user_rated_movies(user_id)\\n        \\n        # Get user statistics\\n        user_mean = self.user_means.get(user_id, self.global_mean)\\n        \\n        # Predict ratings for unrated movies\\n        movie_scores = {}\\n        \\n        for movie_id in self.movies_df['movieId'].unique():\\n            if movie_id not in rated_movies:\\n                movie_mean = self.movie_means.get(movie_id, self.global_mean)\\n                \\n                # Create feature vector\\n                features = {\\n                    'rating_gmean': self.global_mean,\\n                    'user_mean': user_mean,\\n                    'movie_mean': movie_mean,\\n                    'user_bias': user_mean - self.global_mean,\\n                    'movie_bias': movie_mean - self.global_mean,\\n                    'user_movie_bias_interaction': (user_mean - self.global_mean) * (movie_mean - self.global_mean)\\n                }\\n                \\n                # Fill missing features with defaults\\n                feature_vector = np.zeros(len(self.feature_cols))\\n                for i, feature in enumerate(self.feature_cols):\\n                    if feature in features:\\n                        feature_vector[i] = features[feature]\\n                \\n                # Predict rating\\n                if model_name in ['Linear Regression', 'Ridge Regression']:\\n                    feature_vector_scaled = self.scaler.transform(feature_vector.reshape(1, -1))\\n                    predicted_rating = model.predict(feature_vector_scaled)[0]\\n                else:\\n                    predicted_rating = model.predict(feature_vector.reshape(1, -1))[0]\\n                \\n                movie_scores[movie_id] = max(0.5, min(5.0, predicted_rating))\\n        \\n        # Sort and return top recommendations\\n        sorted_movies = sorted(movie_scores.items(), key=lambda x: x[1], reverse=True)\\n        return [movie_id for movie_id, score in sorted_movies[:n_recommendations]]\\n    \\n    def get_popular_movies(self, n_recommendations=10):\\n        \\\"\\\"\\\"Get most popular movies as fallback.\\\"\\\"\\\"\\n        movie_popularity = self.features_df['movieId'].value_counts()\\n        return movie_popularity.head(n_recommendations).index.tolist()\\n    \\n    def get_hybrid_recommendations(self, user_id, n_recommendations=10, \\n                                 weights={'item_based': 0.4, 'ml_based': 0.6}):\\n        \\\"\\\"\\\"Generate hybrid recommendations combining multiple methods.\\\"\\\"\\\"\\n        all_recommendations = {}\\n        \\n        # Item-based recommendations\\n        if 'item_based' in weights:\\n            item_recs = self.recommend_item_based(user_id, n_recommendations * 2)\\n            for i, movie_id in enumerate(item_recs):\\n                score = (len(item_recs) - i) * weights['item_based']\\n                all_recommendations[movie_id] = all_recommendations.get(movie_id, 0) + score\\n        \\n        # ML-based recommendations\\n        if 'ml_based' in weights:\\n            ml_recs = self.recommend_ml_based(user_id, best_model_name, n_recommendations * 2)\\n            for i, movie_id in enumerate(ml_recs):\\n                score = (len(ml_recs) - i) * weights['ml_based']\\n                all_recommendations[movie_id] = all_recommendations.get(movie_id, 0) + score\\n        \\n        # Sort by combined score\\n        sorted_recs = sorted(all_recommendations.items(), key=lambda x: x[1], reverse=True)\\n        return [movie_id for movie_id, score in sorted_recs[:n_recommendations]]\\n    \\n    def format_recommendations(self, movie_ids, method_name):\\n        \\\"\\\"\\\"Format recommendations with movie information.\\\"\\\"\\\"\\n        recommendations = []\\n        for movie_id in movie_ids:\\n            movie_info = self.movies_df[self.movies_df['movieId'] == movie_id]\\n            if len(movie_info) > 0:\\n                movie_data = movie_info.iloc[0]\\n                recommendations.append({\\n                    'movieId': movie_id,\\n                    'title': movie_data['title'],\\n                    'genres': movie_data['genres'],\\n                    'method': method_name,\\n                    'avg_rating': self.movie_means.get(movie_id, self.global_mean)\\n                })\\n        return pd.DataFrame(recommendations)\\n\\n# Initialize recommendation system\\nrec_system = MovieRecommendationSystem(\\n    user_item_matrix=user_item_matrix,\\n    movie_similarity_df=movie_similarity_df,\\n    models=models,\\n    scaler=scaler,\\n    feature_cols=feature_cols,\\n    movies_df=movies_df,\\n    features_df=features_df\\n)\\n\\nprint(\\\"ðŸŽ¯ Movie Recommendation System Initialized!\\\")\\n\\n# Test recommendations for a sample user\\nsample_user_id = user_item_matrix.index[0]  # First user in our matrix\\nprint(f\\\"\\\\nðŸ‘¤ Generating recommendations for User ID: {sample_user_id}\\\")\\n\\n# Get user's rating history\\nuser_rated_movies = rec_system.get_user_rated_movies(sample_user_id)\\nprint(f\\\"   ðŸ“Š User has rated {len(user_rated_movies)} movies\\\")\\n\\nif len(user_rated_movies) > 0:\\n    print(f\\\"\\\\nðŸŽ¬ Sample of user's rated movies:\\\")\\n    sample_rated = user_rated_movies[:5]\\n    for movie_id in sample_rated:\\n        movie_info = movies_df[movies_df['movieId'] == movie_id]\\n        if len(movie_info) > 0:\\n            title = movie_info.iloc[0]['title']\\n            rating = user_item_matrix.loc[sample_user_id, movie_id]\\n            print(f\\\"     {title}: {rating}â­\\\")\\n\\n# Generate recommendations using different methods\\nprint(f\\\"\\\\nðŸŽ¯ Recommendations for User {sample_user_id}:\\\")\\nprint(\\\"=\\\" * 60)\\n\\n# 1. Item-based Collaborative Filtering\\nitem_recs = rec_system.recommend_item_based(sample_user_id, 5)\\nitem_recs_df = rec_system.format_recommendations(item_recs, 'Item-based CF')\\nprint(\\\"\\\\nðŸ”— Item-based Collaborative Filtering:\\\")\\ndisplay(item_recs_df[['title', 'genres', 'avg_rating']].head())\\n\\n# 2. ML-based Recommendations\\nml_recs = rec_system.recommend_ml_based(sample_user_id, best_model_name, 5)\\nml_recs_df = rec_system.format_recommendations(ml_recs, f'ML ({best_model_name})')\\nprint(f\\\"\\\\nðŸ¤– ML-based ({best_model_name}):\\\")\\ndisplay(ml_recs_df[['title', 'genres', 'avg_rating']].head())\\n\\n# 3. Hybrid Recommendations\\nhybrid_recs = rec_system.get_hybrid_recommendations(sample_user_id, 5)\\nhybrid_recs_df = rec_system.format_recommendations(hybrid_recs, 'Hybrid')\\nprint(\\\"\\\\nðŸŽ­ Hybrid Recommendations:\\\")\\ndisplay(hybrid_recs_df[['title', 'genres', 'avg_rating']].head())\\n\\n# Summary and conclusions\\nprint(\\\"\\\\nðŸŽ‰ Movie Recommendation System Complete!\\\")\\nprint(\\\"=\\\" * 50)\\nprint(f\\\"âœ… Successfully built a comprehensive recommendation system with:\\\")\\nprint(f\\\"   ðŸ“Š {len(features_df)} ratings processed\\\")\\nprint(f\\\"   ðŸ‘¥ {features_df['userId'].nunique()} users\\\")\\nprint(f\\\"   ðŸŽ¬ {features_df['movieId'].nunique()} movies\\\")\\nprint(f\\\"   ðŸ”§ {len(feature_cols)} engineered features\\\")\\nprint(f\\\"   ðŸ¤– {len(models)} trained ML models\\\")\\nprint(f\\\"   ðŸ† Best model: {best_model_name} (RMSE: {best_rmse:.4f})\\\")\\nprint(f\\\"   ðŸŽ¯ Hybrid recommendation system combining multiple approaches\\\")\\n\\nprint(f\\\"\\\\nðŸ“ˆ Key Insights:\\\")\\nprint(f\\\"   â€¢ {comparison_df.iloc[0]['Model']} achieved the best performance\\\")\\nprint(f\\\"   â€¢ Feature engineering significantly improved prediction accuracy\\\")\\nprint(f\\\"   â€¢ Hybrid approach combines strengths of different methods\\\")\\nprint(f\\\"   â€¢ System can handle both warm-start and cold-start scenarios\\\")\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
